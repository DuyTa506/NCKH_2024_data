{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def get_files_in_subdirectories(directory):\n",
    "    file_list = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            file_list.append(os.path.join(root, file))\n",
    "    return file_list\n",
    "\n",
    "# Thay đổi đường dẫn tới thư mục của bạn ở đây\n",
    "folder_path = './crawled'\n",
    "files = get_files_in_subdirectories(folder_path)\n",
    "print(len(files))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./crawled\\\\danang.txt',\n",
       " './crawled\\\\diendandulich.txt',\n",
       " './crawled\\\\full_nhandan.txt',\n",
       " './crawled\\\\full_nhandanthoinay.txt',\n",
       " './crawled\\\\vinpearl.jsonl',\n",
       " './crawled\\\\csdl_dulich\\\\co_so_luu_tru.csv',\n",
       " './crawled\\\\csdl_dulich\\\\diem_den_du_lich.txt',\n",
       " './crawled\\\\csdl_dulich\\\\the_thao.txt',\n",
       " './crawled\\\\dulich3mien\\\\all_content_dulich3mien.txt',\n",
       " './crawled\\\\dulich3mien\\\\all_title_dulich3mien.txt',\n",
       " './crawled\\\\dulich3mien\\\\cauhoithuonggap.txt']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fileinput\n",
    "def replace_string_in_files(directory, old_string, new_string):\n",
    "    for root, directories, files in os.walk(directory):\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(root, file_name)\n",
    "            # Đảm bảo tệp đang xử lý là tệp văn bản\n",
    "            if not file_path.endswith(('.txt', '.csv', '.py', '.html', '.css')):\n",
    "                continue\n",
    "            with fileinput.FileInput(file_path, inplace=True) as file:\n",
    "                for line in file:\n",
    "                    print(line.replace(old_string, new_string), end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_string = '=================================================='\n",
    "new_string = '\\n'\n",
    "replace_string_in_files(folder_path, old_string, new_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00,  9.24it/s]\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.08it/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  3.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'diem_den_du_lich', 'passage': 'Title: diem_den_du_lich\\n\\nchiến dịch liên lạc với các đơn vị bộ binh, công binh, pháo binh, cao xạ ở phía trước và các đơn vị kho, trạm của Tổng cục cung cấp, hệ thống quân y, dân công hỏa tuyến ở phía sau mặt trận. Đây là mạng thông tin liên lạc trực tiếp giữa Bộ Chỉ huy chiến dịch với Bộ Chính trị Trung ương Đảng, Bác Hồ.. Từ căn hầm xuyên núi, thông từ lán của Đại tướng Võ Nguyên Giáp đến lán của Thiếu tướng Hoàng Văn Thái, cho tới các điểm khác như nơi làm việc của đoàn cố vấn quân sự Trung Quốc, lán làm việc của Trưởng ban thông tin chiến dịch Hoàng Đạo Thúy… tất cả như vẫn còn như nguyên vẹn dấu ấn của lịch sử... Cạnh nơi làm việc và nghỉ ngơi của Đại tướng là hầm trú ẩn được đào xuyên qua lòng núi. Những lúc quân Pháp ném bom dữ dội, Đại tướng làm việc và nghỉ ngơi trong hầm trú ẩn này. Từ lán Đại tướng Võ Nguyên Giáp thông sang lán Tham mưu trưởng Hoàng Văn Thái và lán cố vấn quân sự Vi Quốc Thanh là một đường hầm dài 69m. Đường hầm cao 1,70m, rộng từ 1 đến 3m, giữa đường hầm có một phòng họp diện tích 18m2 và 5 vị trí đặt máy thông tin liên lạc Nằm cách Sở chỉ huy chiến dịch 300m về phía Đông Bắc là nơi quân và dân ta long trọng tổ chức lễ duyệt binh mừng chiến thắng vào ngày 13/5/1954. Trong Dự án tôn tạo Di tích Sở chỉ huy chiến dịch Điện Biên Phủ, Ban quản lý dự án di tích Điện Biên Phủ đã thực hiện ý tưởng của Đại tướng Võ Nguyên Giáp quy hoạch thành khuôn viên trên nền bãi duyệt binh cũ và đặt một cụm tượng đài nhân kỷ niệm 55 năm ngày chiến thắng Điện Biên Phủ. Cùng với di tích Sở chỉ huy chiến dịch Điện Biên Phủ, cụm Tượng đài chiến thắng tại Công viên Mường Phăng cũng là một điểm dừng chân không thể bỏ qua khi đến với khu di tích Sở chỉ huy chiến dịch Điện Biên Phủ. Người dân địa phương quen gọi nơi này với cái tên thân thiết “Tượng đài mừng công”. Suối nước nóng U Va Bao trùm xung quanh là những ngọn đồi xanh mướt hết sức hùng vĩ, suối khoáng nóng U Va sẽ làm mê hoặc những ai đặt chân đến đây. Được đầu tư khai thác hiệu quả, khu du lịch suối khoáng nóng U Va mang lại cảm giác hết sức thoải mái, thư giãn cho bạn khi ngâm mình trong làn nước ấm ở địa điểm du lịch ấn tượng Điện Biên này. Suối khoáng nóng U Va thuộc xã Noọng Luống, huyệnĐiện Biên. Suối nước nóng này nằm cách thành phố Điện Biên khoảng 15km về phía Tây Nam. Có tổng diện tích khoảng 73.000m2 với dòng suối khoáng nóng tự nhiên cùng nhiệt độ trung bình khoảng 75 đến 85 độ C. Là điểm đến được phát hiện bởi thực dân Pháp từ những năm 1950. Bạn có thể đi đến suối khoáng nóng U Va theo cung đường như sau, từ trung tâm thành phố Điện Biên đi theo đường Tôn Đức Thắng – La Thành – Trần Duy Hưng và Hầm Chui đến DDCT08 tại Mễ Trì, tiếp tục đi thẳng đến ĐCT08. Tiếp tục', 'id': 376, 'len': 586}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def write_csv_records_to_text(csv_file):\n",
    "    filename, _ = os.path.splitext(csv_file)\n",
    "\n",
    "    with open(csv_file, 'r', newline='', encoding='utf-8') as csvfile, \\\n",
    "         open(f\"{filename}.txt\", 'w', encoding='utf-8') as txtfile:\n",
    "        \n",
    "        csv_reader = csv.DictReader(csvfile)\n",
    "        fieldnames = csv_reader.fieldnames\n",
    "\n",
    "        for row in csv_reader:\n",
    "            for field in fieldnames:\n",
    "                txtfile.write(f\"{field.capitalize()}: {row[field]}\\n\")\n",
    "            txtfile.write(\"\\n\\n\")\n",
    "    os.remove(csv_file)\n",
    "    return  os.path.basename(f\"{filename}.txt\")\n",
    "\n",
    "\n",
    "\n",
    "def write_jsonl_to_text(jsonl_file):\n",
    "    filename, _ = os.path.splitext(jsonl_file)\n",
    "\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as jsonlfile, \\\n",
    "         open(f\"{filename}.txt\", 'w', encoding='utf-8') as txtfile:\n",
    "        \n",
    "        for line in jsonlfile:\n",
    "            data = json.loads(line)\n",
    "            for key, value in data.items():\n",
    "                txtfile.write(f\"{key.capitalize()}: {value}\\n\")\n",
    "            txtfile.write(\"\\n\\n\")\n",
    "    os.remove(jsonl_file)\n",
    "    return f\"{filename}.txt\"\n",
    "\n",
    "def clean_file_content(input_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as infile:\n",
    "        lines = infile.readlines()\n",
    "\n",
    "    with open(input_file, 'w', encoding='utf-8') as outfile:\n",
    "        for line in lines:\n",
    "            cleaned_line = line.replace(\"[\", \"\").replace(\"]\", \"\").replace(\"{\", \"\").replace(\"}\", \"\")\n",
    "            outfile.write(cleaned_line)\n",
    "    input_file= os.path.basename(input_file)\n",
    "    return input_file\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=584, window_size=50):\n",
    "    \"\"\"Split a long text into multiple chunks (passages) with manageable sizes.\n",
    "    \n",
    "    Args:\n",
    "        chunk_size (int): Maximum size of a chunk.\n",
    "        window_size (int): Decide how many words are overlapped between two consecutive chunks. Basically #overlapped_words = chunk_size - window_size.\n",
    "    Returns:\n",
    "        str: Multiple chunks of text splitted from initial document text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    num_words = len(words)\n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "\n",
    "    while True:\n",
    "        end_idx = start_idx + chunk_size\n",
    "        chunk = \" \".join(words[start_idx:end_idx])\n",
    "        chunks.append(chunk)\n",
    "        if end_idx >= num_words:\n",
    "            break\n",
    "        start_idx += window_size\n",
    "\n",
    "    return chunks\n",
    "\n",
    "def get_corpus(data_dir=\"data/data_raw10k/\"):\n",
    "    \"\"\"Transform a corpus of documents into a corpus of passages.\n",
    "    \n",
    "    Args:\n",
    "        data_dir (str): directory that contains .txt files, each file contains text content of a Wikipedia page.\n",
    "    Returns:\n",
    "        str: A corpus of chunks splitted from multiple initial documents. Each chunk will contain information about (id, title, passage)\n",
    "    \"\"\"\n",
    "    corpus = []\n",
    "    meta_corpus = []\n",
    "\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in tqdm(files):\n",
    "            filepath = os.path.join(root, filename)\n",
    "            if filename.endswith(\".csv\"):\n",
    "                filename = write_csv_records_to_text(filepath)\n",
    "            elif filename.endswith(\".jsonl\") or filename.endswith(\".json\"):\n",
    "                filename = clean_file_content(write_jsonl_to_text(filepath))\n",
    "            elif filename.endswith(\".txt\"):\n",
    "                title = os.path.splitext(filename)[0]\n",
    "                with open(filepath, \"r\") as f:\n",
    "                    text = f.read().strip()\n",
    "\n",
    "                chunks = split_text_into_chunks(text, chunk_size=584, window_size=150)\n",
    "                chunks = [f\"Title: {title}\\n\\n{chunk}\" for chunk in chunks]\n",
    "                meta_chunks = [{\n",
    "                    \"title\": title,\n",
    "                    \"passage\": chunk,\n",
    "                    \"id\": i,\n",
    "                    \"len\": len(chunk.split())\n",
    "                } for i, chunk in enumerate(chunks)]\n",
    "\n",
    "                corpus.extend(chunks)\n",
    "                meta_corpus.extend(meta_chunks)\n",
    "\n",
    "    return meta_corpus\n",
    "\n",
    "def dump_corpus(meta_corpus):\n",
    "    file_path = \"data/corpus_chunks.jsonl\"\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "    with open(file_path, \"a\", encoding=\"utf-8\") as outfile:\n",
    "        for chunk in meta_corpus:\n",
    "            d = json.dumps(chunk, ensure_ascii=False) + \"\\n\"\n",
    "            outfile.write(d)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    data_dir = \"./crawled/\"\n",
    "    meta_corpus = get_corpus(data_dir)\n",
    "    dump_corpus(meta_corpus)\n",
    "    print(meta_corpus[2000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'httpcore' has no attribute 'SyncHTTPTransport'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtqdm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\googletrans\\__init__.py:6\u001b[0m\n\u001b[0;32m      2\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTranslator\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      3\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mclient\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translator\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconstants\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LANGCODES, LANGUAGES  \u001b[38;5;66;03m# noqa\u001b[39;00m\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\googletrans\\client.py:25\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogletrans\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Translated, Detected\n\u001b[0;32m     22\u001b[0m EXCLUDES \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mca\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslator\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Google Translate ajax API implementation class\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    You have to create an instance of Translator to use this API\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    :type raise_exception: boolean\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, service_urls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_agent\u001b[38;5;241m=\u001b[39mDEFAULT_USER_AGENT,\n\u001b[0;32m     54\u001b[0m                  raise_exception\u001b[38;5;241m=\u001b[39mDEFAULT_RAISE_EXCEPTION,\n\u001b[0;32m     55\u001b[0m                  proxies: typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, httpcore\u001b[38;5;241m.\u001b[39mSyncHTTPTransport] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, timeout: Timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n",
      "File \u001b[1;32md:\\Anaconda\\lib\\site-packages\\googletrans\\client.py:55\u001b[0m, in \u001b[0;36mTranslator\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTranslator\u001b[39;00m:\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Google Translate ajax API implementation class\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    You have to create an instance of Translator to use this API\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    :type raise_exception: boolean\u001b[39;00m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, service_urls\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, user_agent\u001b[38;5;241m=\u001b[39mDEFAULT_USER_AGENT,\n\u001b[0;32m     54\u001b[0m                  raise_exception\u001b[38;5;241m=\u001b[39mDEFAULT_RAISE_EXCEPTION,\n\u001b[1;32m---> 55\u001b[0m                  proxies: typing\u001b[38;5;241m.\u001b[39mDict[\u001b[38;5;28mstr\u001b[39m, \u001b[43mhttpcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSyncHTTPTransport\u001b[49m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, timeout: Timeout \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mClient()\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m proxies \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'httpcore' has no attribute 'SyncHTTPTransport'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from googletrans import Translator\n",
    "import os\n",
    "import time\n",
    "\n",
    "def translate_to_en(text):\n",
    "    translator = Translator()\n",
    "    translation = translator.translate(text, src='vi', dest='en')\n",
    "    return translation.text\n",
    "\n",
    "def translate_files_in_directory(data_dir, new_dir):\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for filename in tqdm(files):\n",
    "            input_file_path = os.path.join(root, filename)\n",
    "            output_file_path = os.path.join(new_dir, f\"en_{filename}\")\n",
    "\n",
    "            with open(input_file_path, 'r', encoding='utf-8') as input_file:\n",
    "                vietnamese_sentences = input_file.readlines()\n",
    "\n",
    "            with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "                for i, sentence in enumerate(vietnamese_sentences):\n",
    "                    if i % 50 == 0 and i != 0:\n",
    "                        time.sleep(120)\n",
    "                    sentence = sentence.strip()\n",
    "                    if sentence: \n",
    "                        vietnamese_translation = translate_to_en(sentence)\n",
    "                        print(vietnamese_translation)\n",
    "                        output_file.write(vietnamese_translation + '\\n')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data_dir = \"E:\\\\AI_Project\\\\travel_data_crawler\\\\crawled\"  # Update this with the directory path containing your files\n",
    "    translate_files_in_directory(data_dir, \"./en_data\")\n",
    "    print(\"Translation completed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
